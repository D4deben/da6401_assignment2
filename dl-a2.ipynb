{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11431344,"sourceType":"datasetVersion","datasetId":7159659},{"sourceId":11435776,"sourceType":"datasetVersion","datasetId":7162968},{"sourceId":341110,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":285317,"modelId":306151}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Installing WandB\n!pip install wandb -qqq","metadata":{"id":"N2pH1fX9EVhL","outputId":"af3350f5-db05-4e9d-a3c4-22e7d8b8834a","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T22:48:46.711042Z","iopub.execute_input":"2025-04-16T22:48:46.711390Z","iopub.status.idle":"2025-04-16T22:48:51.130007Z","shell.execute_reply.started":"2025-04-16T22:48:46.711367Z","shell.execute_reply":"2025-04-16T22:48:51.129019Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", device)\n\nfrom tqdm import tqdm","metadata":{"id":"-_FwRdhCEfzs","outputId":"62d86701-d418-4c0f-e956-ec36d5059853","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T22:48:51.131174Z","iopub.execute_input":"2025-04-16T22:48:51.131474Z","iopub.status.idle":"2025-04-16T22:48:58.484718Z","shell.execute_reply.started":"2025-04-16T22:48:51.131439Z","shell.execute_reply":"2025-04-16T22:48:58.483911Z"}},"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import wandb, os\nos.environ['WANDB_API_KEY'] = \"5203e53880ceb7b6d2c0a93809e14ae43261f2ed\" #your key here\nwandb.login()","metadata":{"id":"ahMJIFQ9Egkh","outputId":"65c0d423-4208-4836-ccde-eb39677b35f4","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T22:48:58.486604Z","iopub.execute_input":"2025-04-16T22:48:58.486923Z","iopub.status.idle":"2025-04-16T22:49:07.121037Z","shell.execute_reply.started":"2025-04-16T22:48:58.486901Z","shell.execute_reply":"2025-04-16T22:49:07.120388Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m016\u001b[0m (\u001b[33mcs24m016-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip install lightning","metadata":{"id":"GbUW9THFFLSt","outputId":"4f2508f7-060f-4b4e-902c-f4af3b322fdc","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:23:31.416452Z","iopub.execute_input":"2025-04-16T07:23:31.417185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 1","metadata":{"id":"1Sa5OfBjM_bV"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ConvNet(nn.Module):\n    def __init__(\n        self,\n        input_shape=(3, 224, 224),\n        conv_filters=[32, 64, 128, 256, 512],\n        filter_sizes=[3, 3, 3, 3, 3],\n        activation_fn=nn.ReLU,\n        dense_units=256,\n        dense_activation_fn=nn.ReLU,\n        dropout_rate=0.3,\n        batch_norm=True,\n        num_classes=10\n    ):\n        super(ConvNet, self).__init__()\n\n        self.conv_blocks = nn.Sequential()\n        in_channels = input_shape[0]\n        h, w = input_shape[1], input_shape[2]\n\n        # Add 5 Conv-BN-Activation-Pool blocks\n        for i in range(5):\n            out_channels = conv_filters[i]\n            kernel_size = filter_sizes[i]\n            padding = kernel_size // 2  # keep same spatial size before pooling\n\n            self.conv_blocks.add_module(f\"conv{i+1}\", nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding))\n            if batch_norm:\n                self.conv_blocks.add_module(f\"bn{i+1}\", nn.BatchNorm2d(out_channels))\n            self.conv_blocks.add_module(f\"act{i+1}\", activation_fn())\n            self.conv_blocks.add_module(f\"pool{i+1}\", nn.MaxPool2d(2))\n            if dropout_rate > 0:\n                self.conv_blocks.add_module(f\"dropout{i+1}\", nn.Dropout2d(dropout_rate))\n\n            in_channels = out_channels\n            h, w = h // 2, w // 2  # due to MaxPool2d(2)\n\n        # Compute the flattened size after conv blocks\n        self.flattened_size = in_channels * h * w\n\n        self.fc1 = nn.Linear(self.flattened_size, dense_units)\n        self.fc1_act = dense_activation_fn()\n        self.dropout = nn.Dropout(dropout_rate)\n\n        self.output_layer = nn.Linear(dense_units, num_classes)\n\n    def forward(self, x):\n        x = self.conv_blocks(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(self.fc1_act(self.fc1(x)))\n        return self.output_layer(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 2","metadata":{}},{"cell_type":"code","source":"!pip install wandb\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\n\ndef get_dataloaders(data_dir, batch_size=64, val_split=0.2, augment=True):\n    # Transforms\n    train_transforms = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ToTensor()\n    ]) if augment else transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor()\n    ])\n\n    test_transforms = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor()\n    ])\n\n    full_dataset = ImageFolder(root=data_dir, transform=train_transforms)\n\n    # Stratified split\n    targets = np.array(full_dataset.targets)\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=val_split, random_state=42)\n    train_idx, val_idx = next(splitter.split(np.zeros(len(targets)), targets))\n\n    train_set = Subset(full_dataset, train_idx)\n    val_set = Subset(ImageFolder(root=data_dir, transform=test_transforms), val_idx)\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader, len(full_dataset.classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nimport wandb\n\ndef train(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n    model.to(device)\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss, correct = 0, 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            correct += (outputs.argmax(dim=1) == labels).sum().item()\n\n        train_accuracy = correct / len(train_loader.dataset)\n\n        # Validation\n        model.eval()\n        val_correct, val_loss = 0, 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n\n        val_accuracy = val_correct / len(val_loader.dataset)\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": total_loss / len(train_loader),\n            \"train_accuracy\": train_accuracy,\n            \"val_loss\": val_loss / len(val_loader),\n            \"val_accuracy\": val_accuracy\n        })\n\n        print(f\"Epoch {epoch+1} - Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import models\nfrom torch import optim\nimport torch.nn as nn\nimport wandb\n\n\ndef main():\n    wandb.init(project=\"DL_A2\")\n\n    config = wandb.config\n\n    activation_map = {\n        \"ReLU\": nn.ReLU,\n        \"GELU\": nn.GELU,\n        \"SiLU\": nn.SiLU,\n        \"Mish\": nn.Mish\n    }\n\n    model = ConvNet(\n        input_shape=(3, 224, 224),\n        conv_filters=config.conv_filters,\n        filter_sizes=config.filter_sizes,\n        activation_fn=activation_map[config.activation_fn],\n        dense_units=config.dense_units,\n        dense_activation_fn=activation_map[config.activation_fn],\n        dropout_rate=config.dropout,\n        batch_norm=config.batch_norm,\n        num_classes=10\n    )\n\n    train_loader, val_loader, _ = get_dataloaders(\n        data_dir=\"/kaggle/input/nature-12k/inaturalist_12K/train\",\n        batch_size=config.batch_size,\n        augment=config.augment\n    )\n\n    optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=0.9, weight_decay=1e-5)\n    criterion = nn.CrossEntropyLoss()\n\n    train(model, train_loader, val_loader, optimizer, criterion, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), epochs=config.epochs)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_config = {\n    \"method\": \"random\",\n    \"metric\": {\n        \"name\": \"val_accuracy\",\n        \"goal\": \"maximize\"\n    },\n    \"parameters\": {\n        \"conv_filters\": {\n            \"values\": [[32, 32, 64, 64, 128], [32, 64, 128, 256, 512]]\n        },\n        \"filter_sizes\": {\n            \"values\": [[3, 3, 3, 3, 3]]\n        },\n        \"activation_fn\": {\n            \"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n        },\n        \"dropout\": {\n            \"values\": [0.2, 0.3]\n        },\n        \"dense_units\": {\n            \"values\": [128, 256]\n        },\n        \"batch_norm\": {\n            \"values\": [True, False]\n        },\n        \"augment\": {\n            \"values\": [True, False]\n        },\n        \"batch_size\": {\n            \"values\": [64, 128]\n        },\n        \"lr\": {\n            \"values\": [0.01, 0.001]\n        },\n        \"epochs\": {\n            \"value\": 10\n        }\n    }\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep=sweep_config, project='DL_A2')\nwandb.agent(sweep_id, function=main, count=5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep=sweep_config, project='DL_A2')\nwandb.agent(sweep_id, function=main, count=50)\n","metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"gpu mem clear","metadata":{}},{"cell_type":"code","source":"!pip install GPUtil\n\nfrom GPUtil import showUtilization as gpu_usage\ngpu_usage()                             \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()                           \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# question 3 ","metadata":{}},{"cell_type":"code","source":"\nsweep_config = {\n    \"method\": \"random\",\n    \"metric\": {\n        \"name\": \"val_accuracy\",\n        \"goal\": \"maximize\"\n    },\n    \"parameters\": {\n        \"conv_filters\": {\n            \"values\": [[32, 32, 64, 64, 128],[512,256,128,64,32],[256,128,64,64,32], [32, 64, 128, 256, 512]]\n        },\n        \"filter_sizes\": {\n            \"values\": [[3, 3, 3, 3, 3],[5,5,5,5,5],[7,7,7,7,7],[7,7,5,5,3],[7,5,3,3,3]]\n        },\n        \"activation_fn\": {\n            \"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n        },\n        \"dropout\": {\n            \"values\": [0.0,0.2, 0.3]\n        },\n        \"dense_units\": {\n            \"values\": [128, 256]\n        },\n        \"batch_norm\": {\n            \"values\": [True]\n        },\n        \"augment\": {\n            \"values\": [True, False]\n        },\n        \"batch_size\": {\n            \"values\": [64, 128,256]\n        },\n        \"lr\": {\n            \"values\": [0.01, 0.001]\n        },\n        \"epochs\": {\n            \"value\": 10\n        }\n    }\n}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\nimport wandb\n\n\ndef get_dataloaders(data_dir, batch_size=256, val_split=0.2, augment=True):\n    # Enhanced data augmentation\n    train_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]) if augment else transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    val_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n    \n    # Stratified split\n    targets = np.array(full_dataset.targets)\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=val_split, random_state=42)\n    train_idx, val_idx = next(splitter.split(np.zeros(len(targets)), targets))\n    \n    train_set = Subset(full_dataset, train_idx)\n    val_set = Subset(datasets.ImageFolder(root=data_dir, transform=val_transforms), val_idx)\n    \n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n                             num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, \n                           num_workers=4, pin_memory=True)\n    \n    return train_loader, val_loader, full_dataset.classes\n\nclass OptimizedCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(OptimizedCNN, self).__init__()\n        \n        # Larger filters in early layers, smaller in later layers\n        self.conv_blocks = nn.Sequential(\n            # Block 1: 64 filters, 7x7 kernel\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 2: 128 filters, 5x5 kernel\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 3: 256 filters, 3x3 kernel\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 4: 512 filters, 3x3 kernel\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 5: 512 filters, 3x3 kernel\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n        \n        # Classifier\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, num_classes))\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, 0, 0.01)\n                init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        x = self.conv_blocks(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\ndef train(model, train_loader, val_loader, optimizer, criterion, scheduler, device, epochs=20):\n    model.to(device)\n    best_val_acc = 0.0\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_correct = 0.0, 0\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            train_correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100 * train_correct / len(train_loader.dataset)\n        \n        # Validation\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                val_correct += outputs.argmax(1).eq(labels).sum().item()\n        \n        val_acc = 100 * val_correct / len(val_loader.dataset)\n        \n        # Step the scheduler\n        scheduler.step(val_loss)\n        \n        # Log metrics\n        wandb.log({            \n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss / len(train_loader),\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss / len(val_loader),\n            \"val_accuracy\": val_acc,\n            \"lr\": optimizer.param_groups[0]['lr']\n        })\n        \n        print(f\"Epoch {epoch+1}/{epochs} - \"\n              f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n              f\"Train Acc: {train_acc:.2f}%, \"\n              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n              f\"Val Acc: {val_acc:.2f}%\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    return best_val_acc\n\ndef main():\n    wandb.init(project=\"DL_A2\")\n    \n    # Device configuration\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Get data loaders\n    train_loader, val_loader, classes = get_dataloaders(\n        data_dir=\"/kaggle/input/nature-12k/inaturalist_12K/train\",\n        batch_size=256,\n        augment=True\n    )\n    \n    # Initialize model\n    model = OptimizedCNN(num_classes=len(classes))\n    \n    # Loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Optimizer with momentum and weight decay\n    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-5)\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    \n    # Train the model\n    best_val_acc = train(\n        model, train_loader, val_loader, \n        optimizer, criterion, scheduler,\n        device=device, epochs=20\n    )\n    \n    wandb.summary[\"best_val_acc\"] = best_val_acc\n    wandb.finish()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep=sweep_config, project='DL_A2')\nwandb.agent(sweep_id, function=main, count=20)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 4","metadata":{}},{"cell_type":"markdown","source":"# Best Model ","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\nimport wandb\n\n\n\nsweep_config = {\n    \"method\": \"random\",\n    \"metric\": {\n        \"name\": \"val_accuracy\",\n        \"goal\": \"maximize\"\n    },\n    \"parameters\": {\n        \"conv_filters\": {\n            \"values\": [[256,128,64,64,32], [32, 64, 128, 256, 512]]\n        },\n        \"filter_sizes\": {\n            \"values\": [[3, 3, 3, 3, 3],[7,7,7,7,7]]\n        },\n        \"activation_fn\": {\n            \"values\": [ \"GELU\"]\n        },\n        \"dropout\": {\n            \"values\": [0.2, 0.3]\n        },\n        \"dense_units\": {\n            \"values\": [128]\n        },\n        \"batch_norm\": {\n            \"values\": [True]\n        },\n        \"augment\": {\n            \"values\": [False]\n        },\n        \"batch_size\": {\n            \"values\": [64,256]\n        },\n        \"lr\": {\n            \"values\": [0.01, 0.001]\n        },\n        \"epochs\": {\n            \"value\": 30\n        }\n    }\n}\n\n\n\ndef get_dataloaders(data_dir, batch_size=256, val_split=0.2, augment=True):\n    # Enhanced data augmentation\n    train_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]) if augment else transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    val_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n    \n    # Stratified split\n    targets = np.array(full_dataset.targets)\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=val_split, random_state=42)\n    train_idx, val_idx = next(splitter.split(np.zeros(len(targets)), targets))\n    \n    train_set = Subset(full_dataset, train_idx)\n    val_set = Subset(datasets.ImageFolder(root=data_dir, transform=val_transforms), val_idx)\n    \n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n                             num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, \n                           num_workers=4, pin_memory=True)\n    \n    return train_loader, val_loader, full_dataset.classes\n\nclass OptimizedCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(OptimizedCNN, self).__init__()\n        \n        # Larger filters in early layers, smaller in later layers\n        self.conv_blocks = nn.Sequential(\n            # Block 1: 64 filters, 7x7 kernel\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 2: 128 filters, 5x5 kernel\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 3: 256 filters, 3x3 kernel\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 4: 512 filters, 3x3 kernel\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            # Block 5: 512 filters, 3x3 kernel\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n        \n        # Classifier\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, num_classes))\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, 0, 0.01)\n                init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        x = self.conv_blocks(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\ndef train(model, train_loader, val_loader, optimizer, criterion, scheduler, device, epochs=20):\n    model.to(device)\n    best_val_acc = 0.0\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_correct = 0.0, 0\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            train_correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100 * train_correct / len(train_loader.dataset)\n        \n        # Validation\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                val_correct += outputs.argmax(1).eq(labels).sum().item()\n        \n        val_acc = 100 * val_correct / len(val_loader.dataset)\n        \n        # Step the scheduler\n        scheduler.step(val_loss)\n        \n        # Log metrics\n        wandb.log({            \n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss / len(train_loader),\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss / len(val_loader),\n            \"val_accuracy\": val_acc,\n            \"lr\": optimizer.param_groups[0]['lr']\n        })\n        \n        print(f\"Epoch {epoch+1}/{epochs} - \"\n              f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n              f\"Train Acc: {train_acc:.2f}%, \"\n              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n              f\"Val Acc: {val_acc:.2f}%\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    return best_val_acc\n\ndef main():\n    wandb.init(project=\"DL_A2\")\n    \n    # Device configuration\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Get data loaders\n    train_loader, val_loader, classes = get_dataloaders(\n        data_dir=\"/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/train\",\n        batch_size=256,\n        augment=True\n    )\n    \n    # Initialize model\n    model = OptimizedCNN(num_classes=len(classes))\n    \n    # Loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Optimizer with momentum and weight decay\n    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-5)\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    \n    # Train the model\n    best_val_acc = train(\n        model, train_loader, val_loader, \n        optimizer, criterion, scheduler,\n        device=device, epochs=30\n    )\n    \n    wandb.summary[\"best_val_acc\"] = best_val_acc\n    wandb.finish()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T22:49:07.121721Z","iopub.execute_input":"2025-04-16T22:49:07.122054Z","iopub.status.idle":"2025-04-16T22:49:07.683825Z","shell.execute_reply.started":"2025-04-16T22:49:07.122034Z","shell.execute_reply":"2025-04-16T22:49:07.683285Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep=sweep_config, project='DL_A2')\nwandb.agent(sweep_id, function=main, count=20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T22:49:07.684501Z","iopub.execute_input":"2025-04-16T22:49:07.684821Z","iopub.status.idle":"2025-04-17T02:12:54.616587Z","shell.execute_reply.started":"2025-04-16T22:49:07.684803Z","shell.execute_reply":"2025-04-17T02:12:54.612777Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: ckiu7skp\nSweep URL: https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lknf7ukr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [256, 128, 64, 64, 32]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [7, 7, 7, 7, 7]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL_A2' when running a sweep."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_224914-lknf7ukr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/lknf7ukr' target=\"_blank\">sandy-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/lknf7ukr' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/lknf7ukr</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Train Loss: 2.2102, Train Acc: 18.24%, Val Loss: 2.3575, Val Acc: 17.00%\nEpoch 2/30 - Train Loss: 2.1465, Train Acc: 21.89%, Val Loss: 2.2811, Val Acc: 22.10%\nEpoch 3/30 - Train Loss: 2.1058, Train Acc: 23.70%, Val Loss: 2.0659, Val Acc: 26.55%\nEpoch 4/30 - Train Loss: 2.0861, Train Acc: 24.87%, Val Loss: 2.0728, Val Acc: 25.90%\nEpoch 5/30 - Train Loss: 2.0563, Train Acc: 25.00%, Val Loss: 2.1041, Val Acc: 23.00%\nEpoch 6/30 - Train Loss: 2.0689, Train Acc: 25.57%, Val Loss: 2.0626, Val Acc: 24.60%\nEpoch 7/30 - Train Loss: 2.0330, Train Acc: 26.59%, Val Loss: 2.1449, Val Acc: 23.90%\nEpoch 8/30 - Train Loss: 2.0199, Train Acc: 26.84%, Val Loss: 2.0000, Val Acc: 28.80%\nEpoch 9/30 - Train Loss: 2.0084, Train Acc: 26.98%, Val Loss: 1.9952, Val Acc: 29.20%\nEpoch 10/30 - Train Loss: 2.0070, Train Acc: 27.67%, Val Loss: 2.1523, Val Acc: 25.50%\nEpoch 11/30 - Train Loss: 2.0068, Train Acc: 28.18%, Val Loss: 1.9711, Val Acc: 30.60%\nEpoch 12/30 - Train Loss: 1.9800, Train Acc: 29.29%, Val Loss: 1.9619, Val Acc: 29.55%\nEpoch 13/30 - Train Loss: 1.9619, Train Acc: 29.92%, Val Loss: 2.0813, Val Acc: 27.65%\nEpoch 14/30 - Train Loss: 1.9452, Train Acc: 30.73%, Val Loss: 1.9181, Val Acc: 31.25%\nEpoch 15/30 - Train Loss: 1.9679, Train Acc: 29.54%, Val Loss: 1.9531, Val Acc: 32.20%\nEpoch 16/30 - Train Loss: 1.9201, Train Acc: 31.68%, Val Loss: 2.0086, Val Acc: 31.20%\nEpoch 17/30 - Train Loss: 1.9101, Train Acc: 31.99%, Val Loss: 2.2198, Val Acc: 25.90%\nEpoch 18/30 - Train Loss: 1.8893, Train Acc: 32.04%, Val Loss: 1.9498, Val Acc: 33.10%\nEpoch 19/30 - Train Loss: 1.8498, Train Acc: 33.84%, Val Loss: 1.8112, Val Acc: 36.10%\nEpoch 20/30 - Train Loss: 1.7697, Train Acc: 37.34%, Val Loss: 1.7531, Val Acc: 39.35%\nEpoch 21/30 - Train Loss: 1.7568, Train Acc: 37.72%, Val Loss: 1.7536, Val Acc: 40.25%\nEpoch 22/30 - Train Loss: 1.7462, Train Acc: 37.53%, Val Loss: 1.7099, Val Acc: 40.35%\nEpoch 23/30 - Train Loss: 1.7323, Train Acc: 38.47%, Val Loss: 1.7214, Val Acc: 39.60%\nEpoch 24/30 - Train Loss: 1.7348, Train Acc: 38.60%, Val Loss: 1.7000, Val Acc: 40.75%\nEpoch 25/30 - Train Loss: 1.7199, Train Acc: 38.58%, Val Loss: 1.7016, Val Acc: 40.70%\nEpoch 26/30 - Train Loss: 1.7225, Train Acc: 38.89%, Val Loss: 1.7158, Val Acc: 40.70%\nEpoch 27/30 - Train Loss: 1.7249, Train Acc: 38.90%, Val Loss: 1.6863, Val Acc: 41.10%\nEpoch 28/30 - Train Loss: 1.7075, Train Acc: 39.84%, Val Loss: 1.6967, Val Acc: 41.10%\nEpoch 29/30 - Train Loss: 1.6949, Train Acc: 39.55%, Val Loss: 1.6858, Val Acc: 40.90%\nEpoch 30/30 - Train Loss: 1.6872, Train Acc: 39.90%, Val Loss: 1.6690, Val Acc: 41.75%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>lr</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▅▄▅▄▄▄▃▂▂▂▂▂▁▁▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▄▃▃▃▄▄▃▅▅▄▅▅▅▄▆▆▇██▇███████</td></tr><tr><td>val_loss</td><td>█▇▅▅▅▅▆▄▄▆▄▄▅▄▄▄▇▄▂▂▂▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>41.75</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train_accuracy</td><td>39.90499</td></tr><tr><td>train_loss</td><td>1.68725</td></tr><tr><td>val_accuracy</td><td>41.75</td></tr><tr><td>val_loss</td><td>1.66899</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sandy-sweep-1</strong> at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/lknf7ukr' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/lknf7ukr</a><br> View project at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_224914-lknf7ukr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5lf2veqv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [256, 128, 64, 64, 32]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 3, 3, 3, 3]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL_A2' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_232245-5lf2veqv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/5lf2veqv' target=\"_blank\">light-sweep-2</a></strong> to <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/5lf2veqv' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/5lf2veqv</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Train Loss: 2.2052, Train Acc: 18.26%, Val Loss: 2.2421, Val Acc: 20.55%\nEpoch 2/30 - Train Loss: 2.1291, Train Acc: 22.17%, Val Loss: 2.1544, Val Acc: 22.30%\nEpoch 3/30 - Train Loss: 2.1044, Train Acc: 23.67%, Val Loss: 2.2156, Val Acc: 20.50%\nEpoch 4/30 - Train Loss: 2.0962, Train Acc: 23.85%, Val Loss: 2.0497, Val Acc: 27.15%\nEpoch 5/30 - Train Loss: 2.0717, Train Acc: 25.15%, Val Loss: 2.0553, Val Acc: 25.00%\nEpoch 6/30 - Train Loss: 2.0498, Train Acc: 25.50%, Val Loss: 2.1130, Val Acc: 26.05%\nEpoch 7/30 - Train Loss: 2.0291, Train Acc: 26.79%, Val Loss: 2.0437, Val Acc: 26.05%\nEpoch 8/30 - Train Loss: 2.0239, Train Acc: 27.22%, Val Loss: 2.0706, Val Acc: 26.75%\nEpoch 9/30 - Train Loss: 2.0424, Train Acc: 26.32%, Val Loss: 2.0312, Val Acc: 26.40%\nEpoch 10/30 - Train Loss: 1.9883, Train Acc: 27.90%, Val Loss: 2.0185, Val Acc: 29.45%\nEpoch 11/30 - Train Loss: 1.9875, Train Acc: 28.47%, Val Loss: 1.9602, Val Acc: 29.70%\nEpoch 12/30 - Train Loss: 1.9915, Train Acc: 28.00%, Val Loss: 1.9426, Val Acc: 31.20%\nEpoch 13/30 - Train Loss: 1.9613, Train Acc: 29.82%, Val Loss: 2.0409, Val Acc: 28.80%\nEpoch 14/30 - Train Loss: 1.9528, Train Acc: 29.53%, Val Loss: 1.9189, Val Acc: 32.15%\nEpoch 15/30 - Train Loss: 1.9149, Train Acc: 31.63%, Val Loss: 1.8759, Val Acc: 32.95%\nEpoch 16/30 - Train Loss: 1.9217, Train Acc: 30.95%, Val Loss: 2.2706, Val Acc: 25.30%\nEpoch 17/30 - Train Loss: 1.9227, Train Acc: 31.24%, Val Loss: 1.9359, Val Acc: 32.40%\nEpoch 18/30 - Train Loss: 1.9067, Train Acc: 31.54%, Val Loss: 1.9098, Val Acc: 34.45%\nEpoch 19/30 - Train Loss: 1.9159, Train Acc: 31.62%, Val Loss: 2.0218, Val Acc: 30.45%\nEpoch 20/30 - Train Loss: 1.8277, Train Acc: 35.13%, Val Loss: 1.7574, Val Acc: 38.90%\nEpoch 21/30 - Train Loss: 1.7701, Train Acc: 37.08%, Val Loss: 1.7314, Val Acc: 40.05%\nEpoch 22/30 - Train Loss: 1.7524, Train Acc: 37.57%, Val Loss: 1.7220, Val Acc: 38.95%\nEpoch 23/30 - Train Loss: 1.7347, Train Acc: 38.38%, Val Loss: 1.7195, Val Acc: 39.60%\nEpoch 24/30 - Train Loss: 1.7349, Train Acc: 38.34%, Val Loss: 1.7082, Val Acc: 39.90%\nEpoch 25/30 - Train Loss: 1.7386, Train Acc: 38.48%, Val Loss: 1.7145, Val Acc: 39.35%\nEpoch 26/30 - Train Loss: 1.7176, Train Acc: 39.25%, Val Loss: 1.6865, Val Acc: 40.35%\nEpoch 27/30 - Train Loss: 1.7097, Train Acc: 39.82%, Val Loss: 1.6970, Val Acc: 38.65%\nEpoch 28/30 - Train Loss: 1.6971, Train Acc: 39.90%, Val Loss: 1.6632, Val Acc: 42.10%\nEpoch 29/30 - Train Loss: 1.6969, Train Acc: 39.52%, Val Loss: 1.6869, Val Acc: 41.40%\nEpoch 30/30 - Train Loss: 1.6973, Train Acc: 39.97%, Val Loss: 1.6662, Val Acc: 40.90%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>lr</td><td>██████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▁▃▂▃▃▃▃▄▄▄▄▅▅▃▅▆▄▇▇▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▇▅▆▆▅▆▅▅▄▄▅▄▃█▄▄▅▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>42.1</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train_accuracy</td><td>39.9675</td></tr><tr><td>train_loss</td><td>1.69726</td></tr><tr><td>val_accuracy</td><td>40.9</td></tr><tr><td>val_loss</td><td>1.66619</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">light-sweep-2</strong> at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/5lf2veqv' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/5lf2veqv</a><br> View project at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_232245-5lf2veqv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 28shrd5m with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [32, 64, 128, 256, 512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 3, 3, 3, 3]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL_A2' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_235603-28shrd5m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/28shrd5m' target=\"_blank\">pious-sweep-3</a></strong> to <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/28shrd5m' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/28shrd5m</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Train Loss: 2.1960, Train Acc: 18.53%, Val Loss: 2.2380, Val Acc: 19.10%\nEpoch 2/30 - Train Loss: 2.1396, Train Acc: 22.02%, Val Loss: 2.1545, Val Acc: 21.85%\nEpoch 3/30 - Train Loss: 2.0930, Train Acc: 23.58%, Val Loss: 2.0760, Val Acc: 25.10%\nEpoch 4/30 - Train Loss: 2.0963, Train Acc: 24.25%, Val Loss: 2.0846, Val Acc: 24.05%\nEpoch 5/30 - Train Loss: 2.0774, Train Acc: 24.58%, Val Loss: 2.0899, Val Acc: 26.75%\nEpoch 6/30 - Train Loss: 2.0560, Train Acc: 25.93%, Val Loss: 2.0589, Val Acc: 26.95%\nEpoch 7/30 - Train Loss: 2.0551, Train Acc: 25.77%, Val Loss: 2.0797, Val Acc: 26.05%\nEpoch 8/30 - Train Loss: 2.0352, Train Acc: 26.69%, Val Loss: 2.0163, Val Acc: 28.85%\nEpoch 9/30 - Train Loss: 2.0245, Train Acc: 26.98%, Val Loss: 1.9758, Val Acc: 30.15%\nEpoch 10/30 - Train Loss: 2.0054, Train Acc: 28.44%, Val Loss: 2.0389, Val Acc: 27.60%\nEpoch 11/30 - Train Loss: 1.9942, Train Acc: 27.83%, Val Loss: 1.9721, Val Acc: 31.35%\nEpoch 12/30 - Train Loss: 1.9766, Train Acc: 29.02%, Val Loss: 1.9423, Val Acc: 30.95%\nEpoch 13/30 - Train Loss: 1.9519, Train Acc: 29.72%, Val Loss: 1.9000, Val Acc: 32.65%\nEpoch 14/30 - Train Loss: 1.9467, Train Acc: 30.20%, Val Loss: 1.9228, Val Acc: 31.70%\nEpoch 15/30 - Train Loss: 1.9470, Train Acc: 30.07%, Val Loss: 2.1028, Val Acc: 27.90%\nEpoch 16/30 - Train Loss: 1.9439, Train Acc: 30.60%, Val Loss: 1.9431, Val Acc: 33.45%\nEpoch 17/30 - Train Loss: 1.9131, Train Acc: 31.68%, Val Loss: 1.9684, Val Acc: 31.45%\nEpoch 18/30 - Train Loss: 1.8256, Train Acc: 35.22%, Val Loss: 1.7641, Val Acc: 38.60%\nEpoch 19/30 - Train Loss: 1.7640, Train Acc: 36.99%, Val Loss: 1.7376, Val Acc: 38.50%\nEpoch 20/30 - Train Loss: 1.7546, Train Acc: 37.12%, Val Loss: 1.7461, Val Acc: 38.15%\nEpoch 21/30 - Train Loss: 1.7603, Train Acc: 37.25%, Val Loss: 1.7354, Val Acc: 38.25%\nEpoch 22/30 - Train Loss: 1.7335, Train Acc: 38.29%, Val Loss: 1.7148, Val Acc: 39.60%\nEpoch 23/30 - Train Loss: 1.7312, Train Acc: 38.15%, Val Loss: 1.7096, Val Acc: 40.15%\nEpoch 24/30 - Train Loss: 1.7406, Train Acc: 38.38%, Val Loss: 1.7180, Val Acc: 39.45%\nEpoch 25/30 - Train Loss: 1.7339, Train Acc: 38.42%, Val Loss: 1.6959, Val Acc: 40.35%\nEpoch 26/30 - Train Loss: 1.7142, Train Acc: 39.47%, Val Loss: 1.7127, Val Acc: 41.20%\nEpoch 27/30 - Train Loss: 1.7046, Train Acc: 39.53%, Val Loss: 1.7064, Val Acc: 40.65%\nEpoch 28/30 - Train Loss: 1.7007, Train Acc: 38.85%, Val Loss: 1.7011, Val Acc: 39.65%\nEpoch 29/30 - Train Loss: 1.7019, Train Acc: 39.53%, Val Loss: 1.7084, Val Acc: 41.40%\nEpoch 30/30 - Train Loss: 1.6806, Train Acc: 40.29%, Val Loss: 1.6504, Val Acc: 42.35%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>lr</td><td>████████████████▂▂▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▄▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▃▂▃▃▃▄▄▄▅▅▅▅▄▅▅▇▇▇▇▇▇▇▇█▇▇██</td></tr><tr><td>val_loss</td><td>█▇▆▆▆▆▆▅▅▆▅▄▄▄▆▄▅▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>42.35</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>40.29254</td></tr><tr><td>train_loss</td><td>1.6806</td></tr><tr><td>val_accuracy</td><td>42.35</td></tr><tr><td>val_loss</td><td>1.6504</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pious-sweep-3</strong> at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/28shrd5m' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/28shrd5m</a><br> View project at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_235603-28shrd5m/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0equy2h8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [32, 64, 128, 256, 512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [7, 7, 7, 7, 7]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL_A2' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_002922-0equy2h8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/0equy2h8' target=\"_blank\">unique-sweep-4</a></strong> to <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/0equy2h8' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/0equy2h8</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Train Loss: 2.1942, Train Acc: 18.10%, Val Loss: 2.2092, Val Acc: 19.35%\nEpoch 2/30 - Train Loss: 2.1247, Train Acc: 22.87%, Val Loss: 2.2178, Val Acc: 23.30%\nEpoch 3/30 - Train Loss: 2.0943, Train Acc: 24.03%, Val Loss: 2.0769, Val Acc: 24.55%\nEpoch 4/30 - Train Loss: 2.0754, Train Acc: 25.30%, Val Loss: 2.0337, Val Acc: 26.75%\nEpoch 5/30 - Train Loss: 2.0541, Train Acc: 25.73%, Val Loss: 2.1464, Val Acc: 23.60%\nEpoch 6/30 - Train Loss: 2.0448, Train Acc: 25.84%, Val Loss: 2.0318, Val Acc: 28.30%\nEpoch 7/30 - Train Loss: 2.0367, Train Acc: 27.17%, Val Loss: 2.0229, Val Acc: 27.05%\nEpoch 8/30 - Train Loss: 2.0085, Train Acc: 27.58%, Val Loss: 2.2619, Val Acc: 25.00%\nEpoch 9/30 - Train Loss: 1.9899, Train Acc: 28.47%, Val Loss: 2.0894, Val Acc: 28.65%\nEpoch 10/30 - Train Loss: 1.9758, Train Acc: 29.09%, Val Loss: 1.9670, Val Acc: 31.25%\nEpoch 11/30 - Train Loss: 1.9655, Train Acc: 29.63%, Val Loss: 1.9811, Val Acc: 31.10%\nEpoch 12/30 - Train Loss: 1.9518, Train Acc: 30.28%, Val Loss: 2.0260, Val Acc: 28.65%\nEpoch 13/30 - Train Loss: 1.9361, Train Acc: 30.94%, Val Loss: 2.0028, Val Acc: 32.05%\nEpoch 14/30 - Train Loss: 1.9313, Train Acc: 31.48%, Val Loss: 1.9013, Val Acc: 31.95%\nEpoch 15/30 - Train Loss: 1.9190, Train Acc: 31.02%, Val Loss: 1.9403, Val Acc: 31.50%\nEpoch 16/30 - Train Loss: 1.9109, Train Acc: 32.57%, Val Loss: 1.8845, Val Acc: 34.45%\nEpoch 17/30 - Train Loss: 1.8816, Train Acc: 33.10%, Val Loss: 1.9495, Val Acc: 33.65%\nEpoch 18/30 - Train Loss: 1.8877, Train Acc: 33.28%, Val Loss: 1.9035, Val Acc: 33.00%\nEpoch 19/30 - Train Loss: 1.8669, Train Acc: 33.78%, Val Loss: 1.9918, Val Acc: 32.10%\nEpoch 20/30 - Train Loss: 1.8825, Train Acc: 32.58%, Val Loss: 1.9851, Val Acc: 31.85%\nEpoch 21/30 - Train Loss: 1.7772, Train Acc: 36.89%, Val Loss: 1.7096, Val Acc: 40.35%\nEpoch 22/30 - Train Loss: 1.7292, Train Acc: 38.39%, Val Loss: 1.7137, Val Acc: 40.90%\nEpoch 23/30 - Train Loss: 1.7258, Train Acc: 39.17%, Val Loss: 1.7039, Val Acc: 41.50%\nEpoch 24/30 - Train Loss: 1.7102, Train Acc: 39.87%, Val Loss: 1.6841, Val Acc: 41.40%\nEpoch 25/30 - Train Loss: 1.6903, Train Acc: 40.03%, Val Loss: 1.6661, Val Acc: 42.20%\nEpoch 26/30 - Train Loss: 1.6900, Train Acc: 40.38%, Val Loss: 1.6636, Val Acc: 43.05%\nEpoch 27/30 - Train Loss: 1.6767, Train Acc: 40.99%, Val Loss: 1.6461, Val Acc: 43.45%\nEpoch 28/30 - Train Loss: 1.6792, Train Acc: 40.76%, Val Loss: 1.6512, Val Acc: 43.15%\nEpoch 29/30 - Train Loss: 1.6738, Train Acc: 40.44%, Val Loss: 1.7046, Val Acc: 41.85%\nEpoch 30/30 - Train Loss: 1.6777, Train Acc: 40.26%, Val Loss: 1.6899, Val Acc: 42.45%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>lr</td><td>███████████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▅▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▃▃▂▄▃▃▄▄▄▄▅▅▅▅▅▅▅▅▇▇▇▇██████</td></tr><tr><td>val_loss</td><td>▇▇▆▅▇▅▅█▆▅▅▅▅▄▄▄▄▄▅▅▂▂▂▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>43.45</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train_accuracy</td><td>40.25503</td></tr><tr><td>train_loss</td><td>1.67766</td></tr><tr><td>val_accuracy</td><td>42.45</td></tr><tr><td>val_loss</td><td>1.68988</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">unique-sweep-4</strong> at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/0equy2h8' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/0equy2h8</a><br> View project at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_002922-0equy2h8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4h9zdgyq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [256, 128, 64, 64, 32]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 3, 3, 3, 3]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL_A2' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_010235-4h9zdgyq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/4h9zdgyq' target=\"_blank\">lively-sweep-5</a></strong> to <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/4h9zdgyq' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/4h9zdgyq</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Train Loss: 2.2035, Train Acc: 18.18%, Val Loss: 2.1724, Val Acc: 22.20%\nEpoch 2/30 - Train Loss: 2.1374, Train Acc: 21.53%, Val Loss: 2.0869, Val Acc: 23.40%\nEpoch 3/30 - Train Loss: 2.0977, Train Acc: 23.50%, Val Loss: 2.0843, Val Acc: 24.95%\nEpoch 4/30 - Train Loss: 2.0892, Train Acc: 23.83%, Val Loss: 2.0459, Val Acc: 25.40%\nEpoch 5/30 - Train Loss: 2.0618, Train Acc: 25.77%, Val Loss: 2.0605, Val Acc: 25.45%\nEpoch 6/30 - Train Loss: 2.0402, Train Acc: 26.43%, Val Loss: 2.0509, Val Acc: 26.30%\nEpoch 7/30 - Train Loss: 2.0423, Train Acc: 27.20%, Val Loss: 1.9934, Val Acc: 28.00%\nEpoch 8/30 - Train Loss: 2.0058, Train Acc: 28.22%, Val Loss: 2.1463, Val Acc: 25.60%\nEpoch 9/30 - Train Loss: 2.0067, Train Acc: 27.74%, Val Loss: 1.9837, Val Acc: 30.00%\nEpoch 10/30 - Train Loss: 1.9863, Train Acc: 28.78%, Val Loss: 2.1871, Val Acc: 25.45%\nEpoch 11/30 - Train Loss: 1.9884, Train Acc: 28.67%, Val Loss: 2.0306, Val Acc: 26.65%\nEpoch 12/30 - Train Loss: 1.9931, Train Acc: 27.84%, Val Loss: 2.0172, Val Acc: 28.75%\nEpoch 13/30 - Train Loss: 1.9485, Train Acc: 29.77%, Val Loss: 2.0522, Val Acc: 28.95%\nEpoch 14/30 - Train Loss: 1.8872, Train Acc: 32.30%, Val Loss: 1.8072, Val Acc: 35.90%\nEpoch 15/30 - Train Loss: 1.8308, Train Acc: 34.85%, Val Loss: 1.7896, Val Acc: 37.40%\nEpoch 16/30 - Train Loss: 1.8135, Train Acc: 35.50%, Val Loss: 1.7819, Val Acc: 37.40%\nEpoch 17/30 - Train Loss: 1.8036, Train Acc: 36.32%, Val Loss: 1.7540, Val Acc: 38.70%\nEpoch 18/30 - Train Loss: 1.7901, Train Acc: 35.97%, Val Loss: 1.7678, Val Acc: 38.40%\nEpoch 19/30 - Train Loss: 1.7849, Train Acc: 36.88%, Val Loss: 1.7782, Val Acc: 37.90%\nEpoch 20/30 - Train Loss: 1.7677, Train Acc: 37.17%, Val Loss: 1.7514, Val Acc: 38.65%\nEpoch 21/30 - Train Loss: 1.7668, Train Acc: 36.67%, Val Loss: 1.8236, Val Acc: 36.75%\nEpoch 22/30 - Train Loss: 1.7602, Train Acc: 37.52%, Val Loss: 1.7359, Val Acc: 38.75%\nEpoch 23/30 - Train Loss: 1.7543, Train Acc: 37.12%, Val Loss: 1.7277, Val Acc: 39.80%\nEpoch 24/30 - Train Loss: 1.7521, Train Acc: 37.55%, Val Loss: 1.7476, Val Acc: 39.25%\nEpoch 25/30 - Train Loss: 1.7578, Train Acc: 37.19%, Val Loss: 1.7382, Val Acc: 39.20%\nEpoch 26/30 - Train Loss: 1.7490, Train Acc: 38.02%, Val Loss: 1.7026, Val Acc: 40.65%\nEpoch 27/30 - Train Loss: 1.7436, Train Acc: 38.43%, Val Loss: 1.7310, Val Acc: 40.25%\nEpoch 28/30 - Train Loss: 1.7197, Train Acc: 38.74%, Val Loss: 1.7464, Val Acc: 39.05%\nEpoch 29/30 - Train Loss: 1.7220, Train Acc: 38.65%, Val Loss: 1.6806, Val Acc: 40.95%\nEpoch 30/30 - Train Loss: 1.7250, Train Acc: 39.39%, Val Loss: 1.8738, Val Acc: 36.80%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>lr</td><td>████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▄▄▄▄▄▄▄▄▅▆▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▇▆▆▆▆▆▅▅▅▅▅▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▂▂▂▃▃▂▄▂▃▃▄▆▇▇▇▇▇▇▆▇█▇▇██▇█▆</td></tr><tr><td>val_loss</td><td>█▇▇▆▆▆▅▇▅█▆▆▆▃▃▂▂▂▂▂▃▂▂▂▂▁▂▂▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>40.95</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train_accuracy</td><td>39.39242</td></tr><tr><td>train_loss</td><td>1.72496</td></tr><tr><td>val_accuracy</td><td>36.8</td></tr><tr><td>val_loss</td><td>1.87376</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lively-sweep-5</strong> at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/4h9zdgyq' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/4h9zdgyq</a><br> View project at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_010235-4h9zdgyq/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zvvvpj24 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [32, 64, 128, 256, 512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [7, 7, 7, 7, 7]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL_A2' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_013557-zvvvpj24</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/zvvvpj24' target=\"_blank\">eternal-sweep-6</a></strong> to <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/zvvvpj24' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/zvvvpj24</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Train Loss: 2.1991, Train Acc: 17.76%, Val Loss: 2.2476, Val Acc: 17.25%\nEpoch 2/30 - Train Loss: 2.1265, Train Acc: 22.19%, Val Loss: 2.2045, Val Acc: 23.10%\nEpoch 3/30 - Train Loss: 2.0889, Train Acc: 23.97%, Val Loss: 2.0915, Val Acc: 26.45%\nEpoch 4/30 - Train Loss: 2.0612, Train Acc: 24.93%, Val Loss: 2.3098, Val Acc: 20.85%\nEpoch 5/30 - Train Loss: 2.0449, Train Acc: 26.57%, Val Loss: 2.0976, Val Acc: 23.95%\nEpoch 6/30 - Train Loss: 2.0451, Train Acc: 26.50%, Val Loss: 2.1063, Val Acc: 25.80%\nEpoch 7/30 - Train Loss: 2.0252, Train Acc: 27.14%, Val Loss: 2.0415, Val Acc: 25.80%\nEpoch 8/30 - Train Loss: 1.9970, Train Acc: 28.38%, Val Loss: 1.9918, Val Acc: 28.50%\nEpoch 9/30 - Train Loss: 1.9847, Train Acc: 28.69%, Val Loss: 1.9931, Val Acc: 29.45%\nEpoch 10/30 - Train Loss: 1.9548, Train Acc: 29.98%, Val Loss: 2.0423, Val Acc: 30.65%\nEpoch 11/30 - Train Loss: 1.9607, Train Acc: 30.00%, Val Loss: 1.9788, Val Acc: 31.40%\nEpoch 12/30 - Train Loss: 1.9523, Train Acc: 29.93%, Val Loss: 1.9415, Val Acc: 30.30%\nEpoch 13/30 - Train Loss: 1.9420, Train Acc: 30.07%, Val Loss: 1.9596, Val Acc: 30.15%\nEpoch 14/30 - Train Loss: 1.9426, Train Acc: 30.28%, Val Loss: 1.9306, Val Acc: 32.35%\nEpoch 15/30 - Train Loss: 1.9132, Train Acc: 31.67%, Val Loss: 2.0207, Val Acc: 30.45%\nEpoch 16/30 - Train Loss: 1.9275, Train Acc: 31.69%, Val Loss: 1.8629, Val Acc: 33.05%\nEpoch 17/30 - Train Loss: 1.8884, Train Acc: 33.05%, Val Loss: 1.9520, Val Acc: 32.25%\nEpoch 18/30 - Train Loss: 1.8759, Train Acc: 33.38%, Val Loss: 1.9035, Val Acc: 33.25%\nEpoch 19/30 - Train Loss: 1.8700, Train Acc: 33.29%, Val Loss: 1.8491, Val Acc: 36.60%\nEpoch 20/30 - Train Loss: 1.8728, Train Acc: 33.24%, Val Loss: 1.8933, Val Acc: 35.45%\nEpoch 21/30 - Train Loss: 1.8565, Train Acc: 34.03%, Val Loss: 1.8646, Val Acc: 35.30%\nEpoch 22/30 - Train Loss: 1.8388, Train Acc: 34.90%, Val Loss: 1.8540, Val Acc: 37.20%\nEpoch 23/30 - Train Loss: 1.8351, Train Acc: 35.07%, Val Loss: 1.8347, Val Acc: 37.15%\nEpoch 24/30 - Train Loss: 1.7982, Train Acc: 36.09%, Val Loss: 1.8953, Val Acc: 35.40%\nEpoch 25/30 - Train Loss: 1.8191, Train Acc: 35.34%, Val Loss: 1.9176, Val Acc: 34.40%\nEpoch 26/30 - Train Loss: 1.8020, Train Acc: 36.48%, Val Loss: 1.9129, Val Acc: 35.30%\nEpoch 27/30 - Train Loss: 1.7835, Train Acc: 36.24%, Val Loss: 2.0068, Val Acc: 33.00%\nEpoch 28/30 - Train Loss: 1.6974, Train Acc: 39.99%, Val Loss: 1.6356, Val Acc: 42.75%\nEpoch 29/30 - Train Loss: 1.6408, Train Acc: 42.04%, Val Loss: 1.6359, Val Acc: 41.50%\nEpoch 30/30 - Train Loss: 1.6414, Train Acc: 41.94%, Val Loss: 1.6207, Val Acc: 43.50%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>lr</td><td>██████████████████████████▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▅▅▆▆▆▆▆▆▆▇██</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▅▅▄▅▄▄▄▄▄▃▃▃▃▃▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▃▂▃▃▃▄▄▅▅▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▅█▇█</td></tr><tr><td>val_loss</td><td>▇▇▆█▆▆▅▅▅▅▅▄▄▄▅▃▄▄▃▄▃▃▃▄▄▄▅▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>43.5</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train_accuracy</td><td>41.94274</td></tr><tr><td>train_loss</td><td>1.64145</td></tr><tr><td>val_accuracy</td><td>43.5</td></tr><tr><td>val_loss</td><td>1.62069</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">eternal-sweep-6</strong> at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/zvvvpj24' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/zvvvpj24</a><br> View project at: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_013557-zvvvpj24/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4p66no5p with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: GELU\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filters: [32, 64, 128, 256, 512]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 3, 3, 3, 3]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL_A2' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_020926-4p66no5p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/4p66no5p' target=\"_blank\">absurd-sweep-7</a></strong> to <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/sweeps/ckiu7skp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/4p66no5p' target=\"_blank\">https://wandb.ai/cs24m016-indian-institute-of-technology-madras/DL_A2/runs/4p66no5p</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Train Loss: 2.2048, Train Acc: 18.15%, Val Loss: 2.3809, Val Acc: 18.35%\nEpoch 2/30 - Train Loss: 2.1295, Train Acc: 22.78%, Val Loss: 2.1052, Val Acc: 23.90%\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#1. Testing the Best Model and Creating Prediction Grid\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torchvision.utils import make_grid\n\ndef test_model(model, test_loader, device, classes):\n    model.eval()\n    test_correct = 0\n    all_preds = []\n    all_images = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            \n            test_correct += preds.eq(labels).sum().item()\n            all_preds.extend(preds.cpu().numpy())\n            all_images.extend(images.cpu())\n            all_labels.extend(labels.cpu().numpy())\n    \n    test_acc = 100 * test_correct / len(test_loader.dataset)\n    print(f'Test Accuracy: {test_acc:.2f}%')\n    \n    return test_acc, all_images, all_labels, all_preds\n\ndef create_prediction_grid(images, labels, preds, classes, n=10):\n    # Create a figure with n rows and 3 columns\n    fig, axes = plt.subplots(n, 3, figsize=(10, 3*n))\n    \n    # Get random indices for samples\n    indices = np.random.choice(len(images), n, replace=False)\n    \n    for i, idx in enumerate(indices):\n        # Original image\n        img = images[idx].permute(1, 2, 0).numpy()\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n        img = np.clip(img, 0, 1)\n        \n        axes[i, 0].imshow(img)\n        axes[i, 0].axis('off')\n        if i == 0:\n            axes[i, 0].set_title('Original Image')\n        \n        # True label\n        true_label = classes[labels[idx]]\n        axes[i, 1].text(0.5, 0.5, f'True: {true_label}', \n                       ha='center', va='center', fontsize=12)\n        axes[i, 1].axis('off')\n        if i == 0:\n            axes[i, 1].set_title('True Label')\n        \n        # Predicted label (color red if wrong, green if correct)\n        pred_label = classes[preds[idx]]\n        color = 'red' if preds[idx] != labels[idx] else 'green'\n        axes[i, 2].text(0.5, 0.5, f'Pred: {pred_label}', \n                        ha='center', va='center', fontsize=12, color=color)\n        axes[i, 2].axis('off')\n        if i == 0:\n            axes[i, 2].set_title('Prediction')\n    \n    plt.tight_layout()\n    plt.savefig('prediction_grid.png', bbox_inches='tight')\n    plt.show()\n\n# Load test data\ntest_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_dataset = datasets.ImageFolder(\n    root=\"/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/val\",  # Using val as test\n    transform=test_transforms\n)\n\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n\n# Load best model\nbest_model = OptimizedCNN(num_classes=len(test_dataset.classes))\nbest_model.load_state_dict(torch.load('/kaggle/input/cnn/pytorch/default/1/best_model.pth'))\nbest_model.to(device)\n\n# Test and create grid\ntest_acc, test_images, test_labels, test_preds = test_model(best_model, test_loader, device, test_dataset.classes)\ncreate_prediction_grid(test_images, test_labels, test_preds, test_dataset.classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:02:33.776412Z","iopub.execute_input":"2025-04-16T20:02:33.776922Z","iopub.status.idle":"2025-04-16T20:02:47.336113Z","shell.execute_reply.started":"2025-04-16T20:02:33.776898Z","shell.execute_reply":"2025-04-16T20:02:47.334991Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"need to change the grid and visualize on wandb","metadata":{}},{"cell_type":"code","source":"# Visualizing First Layer Filters\ndef visualize_first_layer_filters(model, image, save_path='first_layer_filters.png'):\n    # Get first conv layer weights\n    first_conv = model.conv_blocks[0]\n    filters = first_conv.weight.data.cpu().numpy()\n    \n    # Normalize filters to 0-1 for visualization\n    f_min, f_max = filters.min(), filters.max()\n    filters = (filters - f_min) / (f_max - f_min)\n    \n    # Plot filters in 8x8 grid\n    fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n    \n    for i in range(8):\n        for j in range(8):\n            idx = i * 8 + j\n            if idx < filters.shape[0]:  # In case we have less than 64 filters\n                filter_img = filters[idx].transpose(1, 2, 0)\n                axes[i, j].imshow(filter_img)\n                axes[i, j].axis('off')\n            else:\n                axes[i, j].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(save_path, bbox_inches='tight')\n    plt.show()\n\n# Get a random test image\nrandom_idx = np.random.randint(len(test_dataset))\nimage, _ = test_dataset[random_idx]\nimage = image.unsqueeze(0).to(device)\n\n# Visualize filters\nvisualize_first_layer_filters(best_model, image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:42:15.209332Z","iopub.execute_input":"2025-04-16T20:42:15.210016Z","iopub.status.idle":"2025-04-16T20:42:17.246693Z","shell.execute_reply.started":"2025-04-16T20:42:15.209989Z","shell.execute_reply":"2025-04-16T20:42:17.246035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms, datasets\nimport wandb\n\ndef visualize_first_layer(model, test_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load transformation\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load test dataset and get random image\n    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n    random_idx = np.random.randint(0, len(test_dataset))\n    img, label = test_dataset[random_idx]\n    img = img.unsqueeze(0).to(device)  # Add batch dimension\n    \n    # Get the first convolutional layer\n    first_conv = model.conv_blocks[0]\n    num_filters = first_conv.out_channels  # Dynamically get number of filters\n    \n    # Visualize filters\n    filters = first_conv.weight.data.cpu().numpy()\n    \n    # Normalize filters to 0-1 for visualization\n    f_min, f_max = filters.min(), filters.max()\n    filters = (filters - f_min) / (f_max - f_min)\n    \n    # Calculate grid size (square as possible)\n    grid_size = int(np.ceil(np.sqrt(num_filters)))\n    \n    # Plot filters\n    plt.figure(figsize=(12, 12))\n    for i in range(num_filters):\n        plt.subplot(grid_size, grid_size, i+1)\n        # Show first channel only (assuming RGB input)\n        plt.imshow(filters[i, 0], cmap='gray')\n        plt.axis('off')\n    plt.suptitle(f'First Layer Filters ({num_filters} total)', fontsize=16)\n    plt.tight_layout()\n    filters_fig = plt.gcf()\n    \n    # Get feature maps\n    model.eval()\n    with torch.no_grad():\n        feature_maps = first_conv(img)\n    \n    # Normalize feature maps\n    fmaps = feature_maps.squeeze(0).cpu().numpy()\n    fmap_min, fmap_max = fmaps.min(), fmaps.max()\n    fmaps = (fmaps - fmap_min) / (fmap_max - fmap_min)\n    \n    # Plot feature maps\n    plt.figure(figsize=(12, 12))\n    for i in range(num_filters):\n        plt.subplot(grid_size, grid_size, i+1)\n        plt.imshow(fmaps[i], cmap='viridis')\n        plt.axis('off')\n    plt.suptitle(f'Feature Maps ({num_filters} total)', fontsize=16)\n    plt.tight_layout()\n    fmap_fig = plt.gcf()\n    \n    # Show original image (denormalized)\n    img_denorm = img.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n    img_denorm = img_denorm * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n    img_denorm = np.clip(img_denorm, 0, 1)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(img_denorm)\n    plt.title(f'Original Test Image\\nClass: {test_dataset.classes[label]}')\n    plt.axis('off')\n    orig_fig = plt.gcf()\n    \n    # Additional analysis\n    # 1. Filter magnitude distribution\n    filter_magnitudes = torch.norm(first_conv.weight.data, dim=(1, 2, 3)).cpu().numpy()\n    \n    plt.figure(figsize=(10, 5))\n    plt.hist(filter_magnitudes, bins=20)\n    plt.title('Filter Magnitude Distribution')\n    plt.xlabel('Magnitude (L2 norm)')\n    plt.ylabel('Count')\n    magnitude_fig = plt.gcf()\n    \n    # 2. Activation statistics\n    activation_means = feature_maps.mean(dim=(0, 2, 3)).cpu().numpy()\n    activation_max = feature_maps.amax(dim=(0, 2, 3)).cpu().numpy()\n    \n    plt.figure(figsize=(10, 5))\n    plt.bar(range(num_filters), activation_means, alpha=0.5, label='Mean')\n    plt.bar(range(num_filters), activation_max, alpha=0.5, label='Max')\n    plt.title('Feature Map Activation Statistics')\n    plt.xlabel('Filter Index')\n    plt.ylabel('Activation Value')\n    plt.legend()\n    activation_fig = plt.gcf()\n    \n    # Log to wandb\n    wandb.init(project=\"DL_A2\", name=\"filter_visualization\")\n    wandb.log({\n        \"original_image\": wandb.Image(orig_fig),\n        \"first_layer_filters\": wandb.Image(filters_fig),\n        \"feature_maps\": wandb.Image(fmap_fig),\n        \"filter_magnitudes\": wandb.Image(magnitude_fig),\n        \"activation_stats\": wandb.Image(activation_fig),\n        \"selected_class\": test_dataset.classes[label]\n    })\n    \n    plt.close('all')\n    return {\n        \"num_filters\": num_filters,\n        \"filter_magnitudes\": filter_magnitudes,\n        \"activation_means\": activation_means,\n        \"activation_max\": activation_max\n    }\n\n# Load your model\nmodel = OptimizedCNN(num_classes=len(test_dataset.classes))  # Adjust as needed\nmodel.load_state_dict(torch.load('/kaggle/input/cnn/pytorch/default/1/best_model.pth'))\nmodel = model.to(device)\n\n# Run visualization\nresults = visualize_first_layer(\n    model, \n    test_dir='/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/val'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:44:42.248877Z","iopub.execute_input":"2025-04-16T20:44:42.249563Z","iopub.status.idle":"2025-04-16T20:44:46.872356Z","shell.execute_reply.started":"2025-04-16T20:44:42.249542Z","shell.execute_reply":"2025-04-16T20:44:46.871377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# best feature map ","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms, datasets\nimport wandb\n\ndef visualize_first_layer(model, test_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load transformation\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load test dataset and get random image\n    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n    random_idx = np.random.randint(0, len(test_dataset))\n    img, label = test_dataset[random_idx]\n    img = img.unsqueeze(0).to(device)  # Add batch dimension\n    \n    # Get the first convolutional layer\n    first_conv = model.conv_blocks[0]\n    num_filters = first_conv.out_channels  # Dynamically get number of filters\n    \n    # Visualize filters\n    filters = first_conv.weight.data.cpu().numpy()\n    \n    # Normalize filters to 0-1 for visualization\n    f_min, f_max = filters.min(), filters.max()\n    filters = (filters - f_min) / (f_max - f_min)\n    \n    # Calculate grid size (square as possible)\n    grid_size = int(np.ceil(np.sqrt(num_filters)))\n    \n    # Plot filters\n    plt.figure(figsize=(12, 12))\n    for i in range(num_filters):\n        plt.subplot(grid_size, grid_size, i+1)\n        # Show first channel only (assuming RGB input)\n        plt.imshow(filters[i, 0], cmap='gray')\n        plt.axis('off')\n    plt.suptitle(f'First Layer Filters ({num_filters} total)', fontsize=16)\n    plt.tight_layout()\n    filters_fig = plt.gcf()\n    \n    # Get feature maps\n    model.eval()\n    with torch.no_grad():\n        feature_maps = first_conv(img)\n    \n    # Normalize feature maps\n    fmaps = feature_maps.squeeze(0).cpu().numpy()\n    fmap_min, fmap_max = fmaps.min(), fmaps.max()\n    fmaps = (fmaps - fmap_min) / (fmap_max - fmap_min)\n    \n    # Plot feature maps\n    plt.figure(figsize=(12, 12))\n    for i in range(num_filters):\n        plt.subplot(grid_size, grid_size, i+1)\n        plt.imshow(fmaps[i], cmap='viridis')\n        plt.axis('off')\n    plt.suptitle(f'Feature Maps ({num_filters} total)', fontsize=16)\n    plt.tight_layout()\n    fmap_fig = plt.gcf()\n    \n    # Show original image (denormalized)\n    img_denorm = img.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n    img_denorm = img_denorm * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n    img_denorm = np.clip(img_denorm, 0, 1)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(img_denorm)\n    plt.title(f'Original Test Image\\nClass: {test_dataset.classes[label]}')\n    plt.axis('off')\n    orig_fig = plt.gcf()\n    \n    # Additional analysis\n    # 1. Filter magnitude distribution (fixed calculation)\n    filter_magnitudes = torch.norm(first_conv.weight.data.view(num_filters, -1), p=2, dim=1).cpu().numpy()\n    \n    plt.figure(figsize=(10, 5))\n    plt.hist(filter_magnitudes, bins=20)\n    plt.title('Filter Magnitude Distribution')\n    plt.xlabel('Magnitude (L2 norm)')\n    plt.ylabel('Count')\n    magnitude_fig = plt.gcf()\n    \n    # 2. Activation statistics\n    activation_means = feature_maps.mean(dim=(0, 2, 3)).cpu().numpy()\n    activation_max = feature_maps.amax(dim=(0, 2, 3)).cpu().numpy()\n    \n    plt.figure(figsize=(10, 5))\n    plt.bar(range(num_filters), activation_means, alpha=0.5, label='Mean')\n    plt.bar(range(num_filters), activation_max, alpha=0.5, label='Max')\n    plt.title('Feature Map Activation Statistics')\n    plt.xlabel('Filter Index')\n    plt.ylabel('Activation Value')\n    plt.legend()\n    activation_fig = plt.gcf()\n    \n    # Log to wandb\n    wandb.init(project=\"DL_A2\", name=\"filter_visualization\")\n    wandb.log({\n        \"original_image\": wandb.Image(orig_fig),\n        \"first_layer_filters\": wandb.Image(filters_fig),\n        \"feature_maps\": wandb.Image(fmap_fig),\n        \"filter_magnitudes\": wandb.Image(magnitude_fig),\n        \"activation_stats\": wandb.Image(activation_fig),\n        \"selected_class\": test_dataset.classes[label]\n    })\n    \n    plt.close('all')\n    return {\n        \"num_filters\": num_filters,\n        \"filter_magnitudes\": filter_magnitudes,\n        \"activation_means\": activation_means,\n        \"activation_max\": activation_max\n    }\n\n# Load your model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntest_dataset = datasets.ImageFolder(root='/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/val', transform=transforms.ToTensor())\nmodel = OptimizedCNN(num_classes=len(test_dataset.classes))\nmodel.load_state_dict(torch.load('/kaggle/input/cnn/pytorch/default/1/best_model.pth', map_location=device))\nmodel = model.to(device)\n\n# Run visualization\nresults = visualize_first_layer(\n    model, \n    test_dir='/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/val'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:50:50.046278Z","iopub.execute_input":"2025-04-16T20:50:50.046457Z","iopub.status.idle":"2025-04-16T20:50:55.258174Z","shell.execute_reply.started":"2025-04-16T20:50:50.046442Z","shell.execute_reply":"2025-04-16T20:50:55.257645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\nimport wandb\nimport random\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ndef create_enhanced_grid(images, labels, preds, class_names, n_rows=10, n_cols=3, title=\"Predictions\"):\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows*2.5))\n    fig.suptitle(title, fontsize=16, y=1.02)\n    \n    for i in range(n_rows):\n        for j in range(n_cols):\n            idx = i * n_cols + j\n            if idx >= len(images):\n                break\n                \n            ax = axes[i,j]\n            img = images[idx].numpy().transpose((1, 2, 0))\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            img = np.clip((img * std + mean), 0, 1)\n            \n            ax.imshow(img)\n            ax.axis('off')\n            \n            true_label = class_names[labels[idx]]\n            pred_label = class_names[preds[idx]]\n            is_correct = preds[idx] == labels[idx]\n            \n            # More informative title with confidence if available\n            title_color = 'green' if is_correct else 'red'\n            title_text = f\"True: {true_label}\\nPred: {pred_label}\"\n            \n            if is_correct:\n                title_text += \"\\n Correct\"\n            else:\n                title_text += \"\\n Wrong\"\n                \n            ax.set_title(title_text, fontsize=9, color=title_color, pad=2)\n    \n    plt.tight_layout()\n    return fig\n\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    return plt.gcf()\n\ndef evaluate_testset(model_path, test_dir, num_grids=10):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n    loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=4)\n    class_names = dataset.classes\n\n    model = OptimizedCNN(num_classes=len(class_names))\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n\n    # Collect all predictions\n    all_images, all_labels, all_preds = [], [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1).cpu()\n            all_images.extend(images.cpu())\n            all_labels.extend(labels)\n            all_preds.extend(preds)\n\n    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n    wandb.init(project=\"DL_A2\", name=\"test_evaluation\", job_type=\"eval\")\n\n    wandb.log({\"test_accuracy\": accuracy})\n\n    # Class-wise accuracy with zero-division handling\n    class_correct = defaultdict(int)\n    class_total = defaultdict(int)\n    \n    for label, pred in zip(all_labels, all_preds):\n        class_total[label] += 1\n        if label == pred:\n            class_correct[label] += 1\n    \n    # Calculate accuracy only for classes that have samples\n    class_acc = {}\n    for i in range(len(class_names)):\n        if class_total[i] > 0:\n            class_acc[class_names[i]] = 100 * class_correct[i] / class_total[i]\n        else:\n            class_acc[class_names[i]] = float('nan')  # Mark as NaN if no samples\n    \n    # Create wandb table\n    wandb.log({\"class_accuracy\": wandb.Table(\n        columns=[\"Class\", \"Accuracy\", \"Samples\"],\n        data=[[class_names[i], \n              class_acc[class_names[i]], \n              class_total[i]] \n             for i in range(len(class_names))]\n    )})\n\n    # Confusion matrix (only for classes with samples)\n    present_classes = [i for i in range(len(class_names)) if class_total[i] > 0]\n    present_labels = [l for l in all_labels if l in present_classes]\n    present_preds = [p for i, p in enumerate(all_preds) if all_labels[i] in present_classes]\n    \n    if present_classes:\n        cm = confusion_matrix(present_labels, present_preds, labels=present_classes)\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=[class_names[i] for i in present_classes],\n                   yticklabels=[class_names[i] for i in present_classes])\n        plt.title('Confusion Matrix (for classes with samples)')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.xticks(rotation=45, ha='right')\n        plt.yticks(rotation=0)\n        wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n        plt.close()\n\n    # Create prediction grids only for classes with samples\n    present_indices = [i for i in range(len(all_labels)) if all_labels[i] in present_classes]\n    if present_indices:\n        for i in range(min(num_grids, 10)):  # Ensure we don't request more grids than possible\n            sample_size = min(30, len(present_indices))  # 10x3 grid\n            indices = random.sample(present_indices, sample_size)\n            sample_imgs = [all_images[j] for j in indices]\n            sample_labels = [all_labels[j] for j in indices]\n            sample_preds = [all_preds[j] for j in indices]\n\n            fig = create_enhanced_grid(\n                sample_imgs, sample_labels, sample_preds, \n                class_names, n_rows=10, n_cols=3,\n                title=f\"Sample Predictions - Grid {i+1}\"\n            )\n            wandb.log({f\"prediction_grid_{i}\": wandb.Image(fig)})\n            plt.close(fig)\n\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    evaluate_testset(\n        model_path='/kaggle/input/cnn/pytorch/default/1/best_model.pth',\n        test_dir='/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/val',\n        num_grids=10\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:18:12.819427Z","iopub.execute_input":"2025-04-16T20:18:12.820137Z","iopub.status.idle":"2025-04-16T20:18:28.422034Z","shell.execute_reply.started":"2025-04-16T20:18:12.820109Z","shell.execute_reply":"2025-04-16T20:18:28.421434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#   best visualization for test dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\nimport wandb\nimport random\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef create_prediction_grid(images, labels, preds, class_names, n_rows=10, n_cols=3, title=\"Predictions\"):\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows*2))\n    fig.suptitle(title, fontsize=16, y=1.02)\n    \n    for i in range(n_rows):\n        for j in range(n_cols):\n            idx = i * n_cols + j\n            if idx >= len(images):\n                break\n                \n            ax = axes[i,j]\n            img = images[idx].numpy().transpose((1, 2, 0))\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            img = np.clip((img * std + mean), 0, 1)\n            \n            ax.imshow(img)\n            ax.axis('off')\n            \n            true_label = class_names[labels[idx]]\n            pred_label = class_names[preds[idx]]\n            is_correct = preds[idx] == labels[idx]\n            \n            title_color = 'green' if is_correct else 'red'\n            title_text = f\"True: {true_label}\\nPred: {pred_label}\"\n            ax.set_title(title_text, fontsize=9, color=title_color, pad=2)\n    \n    plt.tight_layout()\n    return fig\n\ndef evaluate_testset(model_path, test_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Same transforms as validation\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Load test dataset\n    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n    class_names = test_dataset.classes\n\n    # Load model\n    model = OptimizedCNN(num_classes=len(class_names))\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n\n    # Collect predictions and ground truth\n    all_images = []\n    all_labels = []\n    all_preds = []\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            \n            all_images.extend(images.cpu())\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n\n    # Calculate overall accuracy\n    accuracy = 100 * np.sum(np.array(all_labels) == np.array(all_preds)) / len(all_labels)\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n    # Initialize wandb\n    wandb.init(project=\"DL_A2\", name=\"test_evaluation\", job_type=\"eval\")\n    wandb.log({\"test_accuracy\": accuracy})\n\n    # Calculate class-wise accuracy\n    class_correct = defaultdict(int)\n    class_total = defaultdict(int)\n    \n    for label, pred in zip(all_labels, all_preds):\n        class_total[label] += 1\n        if label == pred:\n            class_correct[label] += 1\n    \n    # Create accuracy table\n    accuracy_table = wandb.Table(columns=[\"Class\", \"Accuracy\", \"Samples\"])\n    for class_idx in range(len(class_names)):\n        if class_total[class_idx] > 0:\n            acc = 100 * class_correct[class_idx] / class_total[class_idx]\n        else:\n            acc = float('nan')\n        accuracy_table.add_data(class_names[class_idx], acc, class_total[class_idx])\n    \n    wandb.log({\"class_accuracy\": accuracy_table})\n\n    # Create confusion matrix (only for classes with samples)\n    present_classes = [c for c in range(len(class_names)) if class_total[c] > 0]\n    if present_classes:\n        cm = confusion_matrix(all_labels, all_preds, labels=present_classes)\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=[class_names[c] for c in present_classes],\n                   yticklabels=[class_names[c] for c in present_classes])\n        plt.title('Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.xticks(rotation=45, ha='right')\n        plt.yticks(rotation=0)\n        wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n        plt.close()\n\n    # Create 10x3 prediction grid\n    num_samples = 30  # 10 rows x 3 columns\n    if len(all_images) >= num_samples:\n        indices = random.sample(range(len(all_images)), num_samples)\n        sample_images = [all_images[i] for i in indices]\n        sample_labels = [all_labels[i] for i in indices]\n        sample_preds = [all_preds[i] for i in indices]\n        \n        grid_fig = create_prediction_grid(\n            sample_images, sample_labels, sample_preds, \n            class_names, title=\"Test Set Predictions (Random Sample)\"\n        )\n        wandb.log({\"prediction_grid\": wandb.Image(grid_fig)})\n        plt.close(grid_fig)\n    else:\n        print(f\"Not enough samples ({len(all_images)}) to create full 10x3 grid\")\n\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    evaluate_testset(\n        model_path='/kaggle/input/cnn/pytorch/default/1/best_model.pth',\n        test_dir='/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/val'\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:52:09.660844Z","iopub.execute_input":"2025-04-16T20:52:09.661084Z","iopub.status.idle":"2025-04-16T20:52:32.514117Z","shell.execute_reply.started":"2025-04-16T20:52:09.661064Z","shell.execute_reply":"2025-04-16T20:52:32.513510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:19:20.653819Z","iopub.execute_input":"2025-04-16T13:19:20.654444Z","iopub.status.idle":"2025-04-16T13:19:20.678407Z","shell.execute_reply.started":"2025-04-16T13:19:20.654419Z","shell.execute_reply":"2025-04-16T13:19:20.677809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport wandb\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\n\n\nclass OptimizedCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(OptimizedCNN, self).__init__()\n\n        self.conv_blocks = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2, padding=1),\n\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2, padding=1),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2, padding=1),\n\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2, padding=1),\n\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2, padding=1)\n        )\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_blocks(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n\ndef load_test_data(test_dir=\"/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/val\", batch_size=256):\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n    dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    return dataset, loader\n\n\ndef evaluate_best_model(best_model_path, test_loader, test_dataset, slider_index=0):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = OptimizedCNN(num_classes=len(test_dataset.classes)).to(device)\n    model.load_state_dict(torch.load(best_model_path, map_location=device))\n    model.eval()\n\n    correct = 0\n    total = 0\n    all_images, all_labels, all_preds = [], [], []\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n\n            all_images.extend(images.cpu())\n            all_labels.extend(labels.cpu())\n            all_preds.extend(preds.cpu())\n\n    test_accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n\n    return all_images, all_labels, all_preds, test_accuracy\n\n\ndef create_prediction_grid(images, labels, preds, class_names, slider_index=0):\n    \"\"\"Returns a 10x3 grid of images with predictions\"\"\"\n    random.seed(slider_index)\n    indices = random.sample(range(len(images)), 30)\n\n    fig, axes = plt.subplots(10, 3, figsize=(10, 25))\n    for i, ax in enumerate(axes.flat):\n        img = images[indices[i]].numpy().transpose((1, 2, 0))\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img = np.clip(img, 0, 1)\n        ax.imshow(img)\n        pred = class_names[preds[indices[i]]]\n        label = class_names[labels[indices[i]]]\n        title = f\"Pred: {pred}\\nTrue: {label}\"\n        ax.set_title(title, color='green' if pred == label else 'red', fontsize=8)\n        ax.axis('off')\n\n    plt.tight_layout()\n    img_path = f\"test_grid_{slider_index}.png\"\n    plt.savefig(img_path, dpi=300)\n    return img_path\n\n\ndef main(best_model_path, slider_index=0):\n    wandb.init(project=\"DL_A2\", name=f\"Test Evaluation - Grid {slider_index}\")\n\n    test_dataset, test_loader = load_test_data()\n    all_images, all_labels, all_preds, test_acc = evaluate_best_model(\n        best_model_path, test_loader, test_dataset, slider_index)\n\n    grid_path = create_prediction_grid(all_images, all_labels, all_preds, test_dataset.classes, slider_index)\n\n    wandb.log({\n        \"test_accuracy\": test_acc,\n        \"prediction_grid\": wandb.Image(grid_path),\n        \"grid_index\": slider_index\n    })\n    wandb.finish()\n\n\n# Call this with different slider values\nfor slider_index in range(0, 10):  # Slider index range from 0 to 9\n    main(best_model_path=\"/kaggle/input/cnn/pytorch/default/1/best_model.pth\", slider_index=slider_index)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:09:20.595528Z","iopub.execute_input":"2025-04-16T20:09:20.596189Z","iopub.status.idle":"2025-04-16T20:14:31.091275Z","shell.execute_reply.started":"2025-04-16T20:09:20.596161Z","shell.execute_reply":"2025-04-16T20:14:31.090589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\nimport wandb\nimport random\nfrom PIL import Image\n\n# Load model architecture\nclass OptimizedCNN(torch.nn.Module):\n    def __init__(self, num_classes=10):\n        super(OptimizedCNN, self).__init__()\n        self.conv_blocks = torch.nn.Sequential(\n            torch.nn.Conv2d(3, 64, 7, 2, 3), torch.nn.BatchNorm2d(64), torch.nn.ReLU(), torch.nn.MaxPool2d(3, 2, 1),\n            torch.nn.Conv2d(64, 128, 5, padding=2), torch.nn.BatchNorm2d(128), torch.nn.ReLU(), torch.nn.MaxPool2d(3, 2, 1),\n            torch.nn.Conv2d(128, 256, 3, padding=1), torch.nn.BatchNorm2d(256), torch.nn.ReLU(), torch.nn.MaxPool2d(3, 2, 1),\n            torch.nn.Conv2d(256, 512, 3, padding=1), torch.nn.BatchNorm2d(512), torch.nn.ReLU(), torch.nn.MaxPool2d(3, 2, 1),\n            torch.nn.Conv2d(512, 512, 3, padding=1), torch.nn.BatchNorm2d(512), torch.nn.ReLU(), torch.nn.MaxPool2d(3, 2, 1)\n        )\n        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(512, 1024),\n            torch.nn.ReLU(),\n            torch.nn.Linear(1024, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_blocks(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n# Visualization Utility\ndef create_grid(images, labels, preds, class_names, title=\"Predictions\"):\n    fig, axes = plt.subplots(10, 3, figsize=(12, 30))\n    for idx, ax in enumerate(axes.flat):\n        img = images[idx].numpy().transpose((1, 2, 0))\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img = np.clip((img * std + mean), 0, 1)\n        ax.imshow(img)\n        ax.axis('off')\n        color = 'green' if preds[idx] == labels[idx] else 'red'\n        ax.set_title(f\"True: {class_names[labels[idx]]}\\nPred: {class_names[preds[idx]]}\", fontsize=8, color=color)\n    fig.suptitle(title, fontsize=16)\n    return fig\n\n# Main Evaluation\ndef evaluate_testset(model_path, test_dir, num_grids=10):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n    loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=4)\n    class_names = dataset.classes\n\n    model = OptimizedCNN(num_classes=len(class_names))\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n\n    # Collect all predictions\n    all_images, all_labels, all_preds = [], [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1).cpu()\n            all_images.extend(images.cpu())\n            all_labels.extend(labels)\n            all_preds.extend(preds)\n\n    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n    wandb.init(project=\"DL_A2\", name=\"evaluate_test\", job_type=\"test_eval\", reinit=True)\n\n    wandb.log({\"test_accuracy\": accuracy})\n\n    # Create random panels of 30 images (10x3) with slider\n    total = len(all_images)\n    slider_images = []\n    for i in range(num_grids):\n        indices = random.sample(range(total), 30)\n        sample_imgs = [all_images[j] for j in indices]\n        sample_labels = [all_labels[j] for j in indices]\n        sample_preds = [all_preds[j] for j in indices]\n\n        fig = create_grid(sample_imgs, sample_labels, sample_preds, class_names, title=f\"Random Grid {i+1}\")\n        grid_path = f\"panel_grid_{i}.png\"\n        fig.savefig(grid_path, dpi=300, bbox_inches='tight')\n        wandb.log({f\"prediction_panel_{i}\": wandb.Image(grid_path)})\n\n    # Panel section slider (index from 0 to 9)\n    wandb.log({\n        \"grid_slider\": wandb.Image(\"panel_grid_0.png\"),\n        \"index_slider\": wandb.Html('<input type=\"range\" min=\"0\" max=\"9\" value=\"0\" step=\"1\">')\n    })\n\n    wandb.finish()\n\n# Run\nif __name__ == \"__main__\":\n    evaluate_testset(\n        model_path='/kaggle/input/cnn/pytorch/default/1/best_model.pth',\n        test_dir='/kaggle/input/d/d4debeniitm/nature-12k/inaturalist_12K/val',\n        num_grids=10  # will generate 10 random 10×3 grids\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:00:15.942013Z","iopub.execute_input":"2025-04-16T20:00:15.942646Z","iopub.status.idle":"2025-04-16T20:02:33.774970Z","shell.execute_reply.started":"2025-04-16T20:00:15.942613Z","shell.execute_reply":"2025-04-16T20:02:33.773621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}